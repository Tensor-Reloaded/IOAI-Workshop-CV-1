{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask intro\n",
    "\n",
    "Multitask learning is a technique in which a single model learns to perform multiple related tasks at the same time.\n",
    "\n",
    "Multitasking involves learning shared representations that can be used for multiple tasks. This is usually done by having a \"backbone\" (usually an encoder-type model) and multiple task-specific heads (decoder-type models or simple MLPs).\n",
    "Each task-specific head is trained separately using a task-specific loss, and the loss also propagates to the backbone.\n",
    "\n",
    "Advantages:\n",
    "* the backbone learns to extract better features from the input\n",
    "* multitask learning is more efficient than training multiple models\n",
    "* may lead to better generalization (similar to transfer learning)\n",
    "* reduces overfitting due to the shared representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have a generic multi task model\n",
    "# The backbone learns the shared representation\n",
    "\n",
    "class GenericMultitaskModel(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, task_heads: List[nn.Module]):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.task_heads = nn.ModuleList(task_heads)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        # backbone: input_space -> latent_shared_representation_space\n",
    "        # The backbone learns a function that transforms input data to a shared representation space with usefull features that can be used by multiple tasks\n",
    "        shared_representation = self.backbone(x)\n",
    "        task_outputs = []\n",
    "        for task_head in self.task_heads:\n",
    "            # task_output: latent_shared_representation_space -> task_output\n",
    "            task_output = task_head(shared_representation)\n",
    "            task_outputs.append(task_output)\n",
    "        return task_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a backbone and multiple task heads, and create a multitask model.\n",
    "\n",
    "We can train the multitask model similar to standard models. We just have to be aware that we have multiple model outputs, one for each task.\n",
    "\n",
    "Therefore, we have to calculate a loss function between the ground truth and the prediction for each task. Then, we average the per-task losses, and backpropagate the mean loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask on MNIST\n",
    "\n",
    "Task 1: classify images into digits from 0 to 9\n",
    "\n",
    "Task 2: classify images into even and odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParityMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, train):\n",
    "        self.dataset = datasets.MNIST(root=\"../data\", train=train, download=True)\n",
    "        self.transforms = v2.Compose(\n",
    "            [\n",
    "                v2.ToImage(),\n",
    "                v2.ToDtype(torch.float32, scale=True),\n",
    "                v2.Normalize([0.1307], [0.3081], inplace=True),\n",
    "                torch.flatten,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image, label = self.dataset[i]\n",
    "        return {\n",
    "            \"data\": self.transforms(image),\n",
    "            \"digit_label\": label,\n",
    "            \"parity_label\": 0 if label % 2 == 0 else 1,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = nn.Sequential(\n",
    "    nn.Linear(784, 100),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "digit_task = nn.Sequential(\n",
    "    nn.Linear(100, 10),\n",
    ")\n",
    "parity_task = nn.Sequential(\n",
    "    nn.Linear(100, 2),\n",
    ")\n",
    "model = GenericMultitaskModel(backbone=backbone, task_heads=[digit_task, parity_task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ParityMNIST(train=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "criterion_1 = nn.CrossEntropyLoss()\n",
    "criterion_2 = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 1: Write the training loop and check that each task loss is decreasing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion_1, criterion_2, iterations):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= iterations:\n",
    "            break\n",
    "\n",
    "        data = batch[\"data\"]\n",
    "        task_1_labels = batch[\"digit_label\"]\n",
    "        task_2_labels = batch[\"parity_label\"]\n",
    "\n",
    "        # Complete the code!\n",
    "        loss1 = ...\n",
    "        loss2 = ...\n",
    "\n",
    "        print(\"Task 1 loss\", loss1.item())\n",
    "        print(\"Task 2 loss\", loss2.item())\n",
    "        print()\n",
    "\n",
    "        # Aggregate the two losses and backpropagate them!\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 loss 2.3236191272735596\n",
      "Task 2 loss 0.732703447341919\n",
      "\n",
      "Task 1 loss 2.310081720352173\n",
      "Task 2 loss 0.7832657098770142\n",
      "\n",
      "Task 1 loss 2.303960084915161\n",
      "Task 2 loss 0.7539554238319397\n",
      "\n",
      "Task 1 loss 2.2776031494140625\n",
      "Task 2 loss 0.6621295213699341\n",
      "\n",
      "Task 1 loss 2.299744129180908\n",
      "Task 2 loss 0.7180918455123901\n",
      "\n",
      "Task 1 loss 2.327526092529297\n",
      "Task 2 loss 0.6933788061141968\n",
      "\n",
      "Task 1 loss 2.2679240703582764\n",
      "Task 2 loss 0.6990024447441101\n",
      "\n",
      "Task 1 loss 2.3500874042510986\n",
      "Task 2 loss 0.682680606842041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloader, optimizer, criterion_1, criterion_2, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something like this:\n",
    "```\n",
    "Task 1 loss 2.3236191272735596\n",
    "Task 2 loss 0.732703447341919\n",
    "\n",
    "Task 1 loss 2.310081720352173\n",
    "Task 2 loss 0.7832657098770142\n",
    "\n",
    "Task 1 loss 2.303960084915161\n",
    "Task 2 loss 0.7539554238319397\n",
    "\n",
    "Task 1 loss 2.2776031494140625\n",
    "Task 2 loss 0.6621295213699341\n",
    "\n",
    "Task 1 loss 2.299744129180908\n",
    "Task 2 loss 0.7180918455123901\n",
    "\n",
    "Task 1 loss 2.327526092529297\n",
    "Task 2 loss 0.6933788061141968\n",
    "\n",
    "Task 1 loss 2.2679240703582764\n",
    "Task 2 loss 0.6990024447441101\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Answer</summary>\n",
    "<br>\n",
    "<pre>\n",
    "def train(model, dataloader, optimizer, criterion_1, criterion_2, iterations):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= iterations:\n",
    "            break\n",
    "        data = batch[\"data\"]\n",
    "        task_1_labels = batch[\"digit_label\"]\n",
    "        task_2_labels = batch[\"parity_label\"]\n",
    "        task_outputs = model(data)\n",
    "        loss1 = criterion_1(task_outputs[0], task_1_labels)\n",
    "        loss2 = criterion_2(task_outputs[1], task_2_labels)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        print(\"Task 1 loss\", loss1.item())\n",
    "        print(\"Task 2 loss\", loss2.item())\n",
    "        print()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the losses are decreasing, so the model is learning both tasks.\n",
    "\n",
    "However, the first loss is decreasing faster, while the 2nd is decreasing slower. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = ...\n",
    "loss2 = ...\n",
    "# Not all tasks are equal!\n",
    "# If we sum multiple losses, the larger loss dominates the training, biasing the model towards that task\n",
    "# Sometimes, one task is more important, so we can give that task more weight\n",
    "# Sometimes, one task is eaiser than the other, so the model learns it faster. We need to scale down that loss to prevent overfit\n",
    "factor1 = ...\n",
    "factor2 = ...\n",
    "loss = factor1 * loss1 + factor2 * loss2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bit_313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
