{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask - the OxfordIIITPet dataset for segmentation and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T17:47:53.877137500Z",
     "start_time": "2025-06-10T17:47:53.826604800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T17:48:01.413406700Z",
     "start_time": "2025-06-10T17:47:53.856662800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import torchvision\n",
    "from torchvision.transforms.v2.functional import hflip\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "num_classes = 37"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T17:48:01.418294100Z",
     "start_time": "2025-06-10T17:48:01.417294900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, split, image_transforms):\n",
    "        self.data = OxfordIIITPet(\n",
    "            root=\"../data\",\n",
    "            download=True,\n",
    "            split=split,\n",
    "            target_types=(\"category\",),\n",
    "        )\n",
    "        self.image_transforms = image_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image, class_label = self.data[i]\n",
    "        image = self.image_transforms(image)\n",
    "        return image, class_label\n",
    "\n",
    "\n",
    "image_transforms_train = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize([img_size, img_size]),\n",
    "    v2.RandomCrop([img_size, img_size], padding=12),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.AutoAugment(),\n",
    "])\n",
    "\n",
    "image_transforms_test = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize([img_size, img_size]),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "cutmix_or_mixup = v2.RandomChoice([\n",
    "    v2.CutMix(num_classes=num_classes),\n",
    "    v2.MixUp(num_classes=num_classes),\n",
    "])\n",
    "\n",
    "train_dataset = ClassificationDataset(\"trainval\", image_transforms_train)\n",
    "test_dataset = ClassificationDataset(\"test\", image_transforms_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=32, drop_last=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T17:48:01.573809600Z",
     "start_time": "2025-06-10T17:48:01.422292400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet18', num_classes=num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True)\n",
    "        if not hasattr(self.backbone, \"fc\") and not hasattr(self.backbone, \"head\"):\n",
    "            raise RuntimeError(\"Backbone not implemented: \" + backbone_name)\n",
    "        if not hasattr(self.backbone, \"fc\"):\n",
    "            self.backbone.head = nn.Linear(self.backbone.head.weight.size(1), num_classes)\n",
    "        else:\n",
    "            self.backbone.fc = nn.Linear(self.backbone.fc.weight.size(1), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        self.backbone.requires_grad_(False)\n",
    "        if not hasattr(self.backbone, \"fc\"):\n",
    "            self.backbone.head.requires_grad_(True)\n",
    "        else:\n",
    "            self.backbone.fc.requires_grad_(True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T17:53:24.009683800Z",
     "start_time": "2025-06-10T17:53:24.002648900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def f1_score(x, y):\n",
    "    x_sum = x.sum().item()\n",
    "    y_sum = y.sum().item()\n",
    "\n",
    "    if x_sum == y_sum == 0:\n",
    "        return 1.0\n",
    "    elif x_sum == 0 or y_sum == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2.0 * (x & y).sum().item() / (x_sum + y_sum)\n",
    "\n",
    "\n",
    "def f1_macro(predicted, targets, num_classes):\n",
    "    f1s = []\n",
    "    for cls in range(num_classes):\n",
    "        f1s.append(f1_score(predicted == cls, targets == cls))\n",
    "    return sum(f1s) / num_classes\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T17:48:01.585248600Z",
     "start_time": "2025-06-10T17:48:01.581870100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device\", device)\n",
    "# model = ClassificationModel().to(device)\n",
    "model = ClassificationModel(\"hf_hub:timm/resnest14d.gluon_in1k\").to(device)\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T17:48:02.684548600Z",
     "start_time": "2025-06-10T17:48:01.588251100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train():\n",
    "    loss_sum = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        images, labels = cutmix_or_mixup(images, labels)\n",
    "\n",
    "        with torch.autocast(device.type, enabled=device.type == 'cuda'):\n",
    "            class_logits = model(images)\n",
    "            loss = classification_criterion(class_logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_sum += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "        })\n",
    "\n",
    "    return loss_sum / num_batches\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def val():\n",
    "    cls_f1_sum = 0.0\n",
    "    cls_acc_sum = 0.0\n",
    "    loss_sum = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    model.eval()\n",
    "    pbar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.autocast(device.type, enabled=device.type == 'cuda'):\n",
    "            class_logits = model(images)\n",
    "            loss = classification_criterion(class_logits, labels)\n",
    "\n",
    "            pred_classes = class_logits.argmax(dim=1)\n",
    "\n",
    "            cls_f1 = f1_macro(pred_classes, labels, num_classes=num_classes)\n",
    "            cls_acc = (pred_classes == labels).float().mean().item()\n",
    "\n",
    "        cls_f1_sum += cls_f1\n",
    "        cls_acc_sum += cls_acc\n",
    "        loss_sum += loss\n",
    "        num_batches += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'ClsF1': f'{cls_f1:.4f}',\n",
    "            'ClsAcc': f'{cls_acc:.4f}',\n",
    "        })\n",
    "\n",
    "    return (\n",
    "        cls_f1_sum / num_batches,\n",
    "        cls_acc_sum / num_batches,\n",
    "        loss_sum / num_batches,\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def val_tta(tta_type):\n",
    "    cls_f1_sum = 0.0\n",
    "    cls_acc_sum = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    model.eval()\n",
    "    pbar = tqdm(test_loader, desc=f\"Evaluating with TTA level {level}\")\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.autocast(device.type, enabled=device.type == 'cuda'):\n",
    "            combined = [images]\n",
    "            if tta_type == \"mirroring\":\n",
    "                combined.append(hflip(images))\n",
    "            elif tta_type == \"translate\":  # left\n",
    "                padding_size = 2\n",
    "                padded = v2.functional.pad(images, [padding_size])\n",
    "                for i in [-2, 0, 2]:\n",
    "                    for j in [-2, 0, 2]:\n",
    "                        if i == 0 and j == 0:\n",
    "                            continue\n",
    "                        x = padding_size + i\n",
    "                        y = padding_size + j\n",
    "                        combined.append(padded[:, :, x:x + img_size, y:y + img_size])\n",
    "            elif tta_type == \"mirroring_and_translate\":\n",
    "                combined.append(hflip(images))\n",
    "                padding_size = 2\n",
    "                padded = v2.functional.pad(images, [padding_size])\n",
    "                for i in [-2, 0, 2]:\n",
    "                    for j in [-2, 0, 2]:\n",
    "                        if i == 0 and j == 0:\n",
    "                            continue\n",
    "                        x = padding_size + i\n",
    "                        y = padding_size + j\n",
    "                        aux = padded[:, :, x:x + img_size, y:y + img_size]\n",
    "                        combined.append(aux)\n",
    "                        combined.append(hflip(aux))\n",
    "            elif tta_type == \"translate_aggressive\":\n",
    "                padding_size = 4\n",
    "                padded = v2.functional.pad(images, [padding_size])\n",
    "                for i in [-4, -2, 0, 2, 4]:\n",
    "                    for j in [-4, -2, 0, 2, 4]:\n",
    "                        if i == 0 and j == 0:\n",
    "                            continue\n",
    "                        x = padding_size + i\n",
    "                        y = padding_size + j\n",
    "                        combined.append(padded[:, :, x:x + img_size, y:y + img_size])\n",
    "\n",
    "            outputs = sum(model(x) for x in combined)\n",
    "        outputs = outputs.argmax(dim=1)\n",
    "\n",
    "        cls_f1 = f1_macro(outputs, labels, num_classes=num_classes)\n",
    "        cls_acc = (outputs == labels).float().mean().item()\n",
    "\n",
    "        cls_f1_sum += cls_f1\n",
    "        cls_acc_sum += cls_acc\n",
    "        num_batches += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'ClsF1': f'{cls_f1:.4f}',\n",
    "            'ClsAcc': f'{cls_acc:.4f}',\n",
    "        })\n",
    "\n",
    "    return (\n",
    "        cls_f1_sum / num_batches,\n",
    "        cls_acc_sum / num_batches,\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T17:48:02.702154700Z",
     "start_time": "2025-06-10T17:48:02.694828500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"hf_hub:timm/resnet18.a1_in1k\",\n",
    "    \"hf_hub:timm/resnet50.a1h_in1k\",\n",
    "    \"hf_hub:timm/resnest14d.gluon_in1k\",\n",
    "    \"hf_hub:timm/resnest26d.gluon_in1k\",\n",
    "    \"hf_hub:timm/vit_base_patch16_clip_224.openai\",\n",
    "    \"hf_hub:timm/vit_small_patch16_224.augreg_in21k\",\n",
    "    \"hf_hub:timm/maxvit_tiny_tf_224.in1k\",\n",
    "    \"hf_hub:timm/maxvit_small_tf_224.in1k\",\n",
    "]\n",
    "# Loading all models first\n",
    "for model_name in models:\n",
    "    model = ClassificationModel(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2a5f73ed1a8404ab2d8467ea3f5eb9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 115/115 [01:46<00:00,  1.08it/s, Loss=1.9354]\n",
      "Evaluating:  81%|████████  | 93/115 [01:26<00:20,  1.08it/s, Loss=0.9837, ClsF1=0.9437, ClsAcc=0.8438]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m tr_loss \u001B[38;5;241m=\u001B[39m train()\n\u001B[1;32m---> 20\u001B[0m vl_cls_f1, vl_cls_acc, vl_loss \u001B[38;5;241m=\u001B[39m \u001B[43mval\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Train]                                | Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtr_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Val]   ClsF1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvl_cls_f1\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | ClsAcc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvl_cls_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvl_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 49\u001B[0m, in \u001B[0;36mval\u001B[1;34m()\u001B[0m\n\u001B[0;32m     45\u001B[0m     loss \u001B[38;5;241m=\u001B[39m classification_criterion(class_logits, labels)\n\u001B[0;32m     47\u001B[0m     pred_classes \u001B[38;5;241m=\u001B[39m class_logits\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 49\u001B[0m     cls_f1 \u001B[38;5;241m=\u001B[39m \u001B[43mf1_macro\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_classes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_classes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m     cls_acc \u001B[38;5;241m=\u001B[39m (pred_classes \u001B[38;5;241m==\u001B[39m labels)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     52\u001B[0m cls_f1_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m cls_f1\n",
      "Cell \u001B[1;32mIn[6], line 16\u001B[0m, in \u001B[0;36mf1_macro\u001B[1;34m(predicted, targets, num_classes)\u001B[0m\n\u001B[0;32m     14\u001B[0m f1s \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_classes):\n\u001B[1;32m---> 16\u001B[0m     f1s\u001B[38;5;241m.\u001B[39mappend(\u001B[43mf1_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredicted\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msum\u001B[39m(f1s) \u001B[38;5;241m/\u001B[39m num_classes\n",
      "Cell \u001B[1;32mIn[6], line 2\u001B[0m, in \u001B[0;36mf1_score\u001B[1;34m(x, y)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf1_score\u001B[39m(x, y):\n\u001B[1;32m----> 2\u001B[0m     x_sum \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     y_sum \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x_sum \u001B[38;5;241m==\u001B[39m y_sum \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model_name in models:\n",
    "    model = ClassificationModel(model_name).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    model.freeze_backbone()\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        tr_loss = train()\n",
    "        vl_cls_f1, vl_cls_acc, vl_loss = val()\n",
    "        print(f\"[Train]                                | Loss: {tr_loss:.4f}\")\n",
    "        print(f\"[Val]   ClsF1: {vl_cls_f1:.4f} | ClsAcc: {vl_cls_acc:.4f} | Loss: {vl_loss:.4f}\")\n",
    "\n",
    "    for level in [\"no_tta\", \"mirroring\", \"translate\"]:\n",
    "        val_f1_tta, val_acc_tta = val_tta(level)\n",
    "        results.append((model_name, level, val_f1_tta, val_acc_tta))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T18:01:34.090554200Z",
     "start_time": "2025-06-10T17:55:45.839890900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model_name, level, val_f1_tta, val_acc_tta in results:\n",
    "    print(f\"{model_name.split('/')[-1]: <30} | {level: <10} | ClsF1: {val_f1_tta:.4f} | ClsAcc: {val_acc_tta:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bit_313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
